{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Step 0 bis Step 7 + Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import missingno as msno\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         id  age_of_customer     sex       ctry         town  \\\n",
       "0    C0001             60.0   Other    Germany      Hamburg   \n",
       "1    C0002             53.0    Male    Germayn       Berlin   \n",
       "2    C0003             30.0   Other         UK       London   \n",
       "3    C0004             24.0   Other    Germany      Hamburg   \n",
       "4    C0005             53.0    Male        USA  Los Angeles   \n",
       "..     ...              ...     ...        ...          ...   \n",
       "495  C0495             43.0  Female         UK   Birmingham   \n",
       "496  C0496             60.0  Female        USA      Chicago   \n",
       "497  C0498             44.0    Male      India        Delhi   \n",
       "498  C0499             46.0  Female         UK       London   \n",
       "499  C0500             64.0   Other  Australia     Brisbane   \n",
       "\n",
       "     swimming_hours_per_week  biking_hours_per_week  running_hours_per_week  \\\n",
       "0                       1.80                   4.44                    0.34   \n",
       "1                       1.77                   5.42                    5.60   \n",
       "2                       4.05                   6.98                    4.03   \n",
       "3                       3.22                  10.54                    4.23   \n",
       "4                       3.15                   3.03                    5.40   \n",
       "..                       ...                    ...                     ...   \n",
       "495                     4.55                   0.86                    5.19   \n",
       "496                     1.18                  10.67                    7.19   \n",
       "497                     1.07                   3.80                    2.87   \n",
       "498                     0.00                   8.81                    6.31   \n",
       "499                     2.86                   1.08                    8.19   \n",
       "\n",
       "     total_training_hours_per_week  vo2_max  ...  calories_burned_per_week  \\\n",
       "0                             6.58    23.02  ...                   3062.83   \n",
       "1                            12.79    52.46  ...                   6651.29   \n",
       "2                            15.07    73.21  ...                   7506.12   \n",
       "3                            17.99    74.64  ...                   9134.26   \n",
       "4                            11.58    51.22  ...                   5709.64   \n",
       "..                             ...      ...  ...                       ...   \n",
       "495                          10.60    59.16  ...                   5579.17   \n",
       "496                          19.04    44.23  ...                   9713.87   \n",
       "497                           7.73    55.21  ...                   3679.10   \n",
       "498                          15.12    55.85  ...                   7337.50   \n",
       "499                          12.14      NaN  ...                   5850.79   \n",
       "\n",
       "     support_cases_of_customer  customer_years  most_current_software_update  \\\n",
       "0                            3               4                    2023-12-05   \n",
       "1                            2               4                    2024-04-23   \n",
       "2                            0               6                    2024-12-30   \n",
       "3                            3               6                    2023-10-30   \n",
       "4                            3               8                    2024-12-23   \n",
       "..                         ...             ...                           ...   \n",
       "495                          2               3                    2024-06-09   \n",
       "496                          4               5                    2024-07-20   \n",
       "497                          2               1                    2024-04-04   \n",
       "498                          1               1                    2024-05-15   \n",
       "499                          2               4                    2024-10-26   \n",
       "\n",
       "    goal_of_training preferred_training_daytime subscription_type  \\\n",
       "0         Recreation                    Evening           Premium   \n",
       "1         Recreation                    Evening              Free   \n",
       "2        Competition                    Morning              Free   \n",
       "3        Competition                    Morning           Premium   \n",
       "4            Fitness                  Afternoon              Free   \n",
       "..               ...                        ...               ...   \n",
       "495          Fitness                  Afternoon           Premium   \n",
       "496      Competition                    Morning             Basic   \n",
       "497          Fitness                    Morning              Free   \n",
       "498          Fitness                    Morning             Basic   \n",
       "499          Fitness                  Afternoon             Basic   \n",
       "\n",
       "    color_of_watch synchronisation user_of_latest_model  \n",
       "0            Black              No                    0  \n",
       "1            White             Yes                    0  \n",
       "2            Black             Yes                    1  \n",
       "3            White              No                    1  \n",
       "4            Black             Yes                    0  \n",
       "..             ...             ...                  ...  \n",
       "495          Black              No                    1  \n",
       "496          White             Yes                    0  \n",
       "497          Black             Yes                    0  \n",
       "498          Black              No                    0  \n",
       "499          Black             Yes                    0  \n",
       "\n",
       "[500 rows x 21 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/raw/triathlon_watch_test_data_final.csv')\n",
    "# Rename columns for better handling\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalte ID wird das f√ºhrende \"C\" entfernt und die f√ºhrenden Nullen\n",
    "df[\"id\"] = df[\"id\"].dropna().str.replace(\"C\", \"\", regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  most_current_software_update  days_since_latest_update\n",
      "0                   2023-12-05                       391\n",
      "1                   2024-04-23                       251\n",
      "2                   2024-12-30                         0\n",
      "3                   2023-10-30                       427\n",
      "4                   2024-12-23                         7\n"
     ]
    }
   ],
   "source": [
    "# Sicherstellen, dass die Spalte als Datumsformat erkannt wird\n",
    "df[\"most_current_software_update\"] = pd.to_datetime(df[\"most_current_software_update\"], format=\"%Y-%m-%d\")\n",
    "# ‚úÖ Differenz zum j√ºngsten Datum berechnen (in Tagen)\n",
    "latest_date = df[\"most_current_software_update\"].max()\n",
    "df[\"days_since_latest_update\"] = (latest_date - df[\"most_current_software_update\"]).dt.days\n",
    "# Ergebnis ausgeben\n",
    "print(df[[\"most_current_software_update\", \"days_since_latest_update\"]].head())\n",
    "df = df.drop(columns=[\"most_current_software_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric=df.select_dtypes(include=['float64']).columns.tolist() #list of numeric columns\n",
    "categorical = df.select_dtypes(include=['object']).columns.to_list() #append all categorical columns to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                               float64\n",
      "age_of_customer                  float64\n",
      "sex                               object\n",
      "ctry                              object\n",
      "town                              object\n",
      "swimming_hours_per_week          float64\n",
      "biking_hours_per_week            float64\n",
      "running_hours_per_week           float64\n",
      "total_training_hours_per_week    float64\n",
      "vo2_max                          float64\n",
      "10k_running_time_prediction      float64\n",
      "calories_burned_per_week         float64\n",
      "support_cases_of_customer          int64\n",
      "customer_years                     int64\n",
      "goal_of_training                  object\n",
      "preferred_training_daytime        object\n",
      "subscription_type                 object\n",
      "color_of_watch                    object\n",
      "synchronisation                   object\n",
      "user_of_latest_model               int64\n",
      "days_since_latest_update           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bis hierin wurde nur ganz einfache Ver√§nderungen vollzogen. df ist nun die Grundlage f√ºr die weietren Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step0 =df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_column = \"user_of_latest_model\" if \"user_of_latest_model\" in df_step0.columns else df.columns[-1]\n",
    "# Zeilen mit fehlenden Werten in der Zielvariable entfernen\n",
    "df_step0 = df_step0.dropna(subset=[target_column])\n",
    "# Zeilen mit mehr als zwei fehlenden Werten entfernen\n",
    "df_step0 = df_step0[df_step0.isnull().sum(axis=1) <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 abgeschlossen. Bereinigte Daten gespeichert unter: triathlon_watch_training_data_step0.csv\n",
      "    id  age_of_customer    sex     ctry         town  swimming_hours_per_week  \\\n",
      "0  1.0             60.0  Other  Germany      Hamburg                     1.80   \n",
      "1  2.0             53.0   Male  Germayn       Berlin                     1.77   \n",
      "2  3.0             30.0  Other       UK       London                     4.05   \n",
      "3  4.0             24.0  Other  Germany      Hamburg                     3.22   \n",
      "4  5.0             53.0   Male      USA  Los Angeles                     3.15   \n",
      "\n",
      "   biking_hours_per_week  running_hours_per_week  \\\n",
      "0                   4.44                    0.34   \n",
      "1                   5.42                    5.60   \n",
      "2                   6.98                    4.03   \n",
      "3                  10.54                    4.23   \n",
      "4                   3.03                    5.40   \n",
      "\n",
      "   total_training_hours_per_week  vo2_max  ...  calories_burned_per_week  \\\n",
      "0                           6.58    23.02  ...                   3062.83   \n",
      "1                          12.79    52.46  ...                   6651.29   \n",
      "2                          15.07    73.21  ...                   7506.12   \n",
      "3                          17.99    74.64  ...                   9134.26   \n",
      "4                          11.58    51.22  ...                   5709.64   \n",
      "\n",
      "   support_cases_of_customer  customer_years  goal_of_training  \\\n",
      "0                          3               4        Recreation   \n",
      "1                          2               4        Recreation   \n",
      "2                          0               6       Competition   \n",
      "3                          3               6       Competition   \n",
      "4                          3               8           Fitness   \n",
      "\n",
      "  preferred_training_daytime subscription_type color_of_watch synchronisation  \\\n",
      "0                    Evening           Premium          Black              No   \n",
      "1                    Evening              Free          White             Yes   \n",
      "2                    Morning              Free          Black             Yes   \n",
      "3                    Morning           Premium          White              No   \n",
      "4                  Afternoon              Free          Black             Yes   \n",
      "\n",
      "  user_of_latest_model  days_since_latest_update  \n",
      "0                    0                       391  \n",
      "1                    0                       251  \n",
      "2                    1                         0  \n",
      "3                    1                       427  \n",
      "4                    0                         7  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fehlende Werte analysieren\n",
    "missing_values = df_step0.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Fehlende Werte durch Durchschnitt (numerisch) oder Modus (kategorisch) ersetzen\n",
    "#df_step0 = df_step0.copy()\n",
    "\n",
    "numeric_cols = df_step0.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df_step0.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_step0[col] = df_step0[col].fillna(df_step0[col].mean())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_step0[col] = df_step0[col].fillna(df_step0[col].mode()[0])\n",
    "\n",
    "# Speichern der bereinigten Daten\n",
    "output_path = \"triathlon_watch_training_data_step0.csv\"\n",
    "df_step0.to_csv(output_path, index=False)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(\"Step 0 abgeschlossen. Bereinigte Daten gespeichert unter:\", output_path)\n",
    "print(df_step0.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Numerische Spalten: ['id', 'age_of_customer', 'swimming_hours_per_week', 'biking_hours_per_week', 'running_hours_per_week', 'total_training_hours_per_week', 'vo2_max', '10k_running_time_prediction', 'calories_burned_per_week', 'support_cases_of_customer', 'customer_years', 'user_of_latest_model', 'days_since_latest_update']\n",
      "üî† Kategoriale Spalten: ['sex', 'ctry', 'town', 'goal_of_training', 'preferred_training_daytime', 'subscription_type', 'color_of_watch', 'synchronisation']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = df_step0.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df_step0.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(\"üìä Numerische Spalten:\", numerical_columns)\n",
    "print(\"üî† Kategoriale Spalten:\", categorical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression als Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test = pd.DataFrame(columns=['arbeitsschritt', 'Accuracy', 'F1-Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def run_logistic_regression(df, arbeitsschritt):\n",
    "    global df_score_test\n",
    "\n",
    "    # **Extrahiere die erste Zahl aus arbeitsschritt**\n",
    "    step_number = str(arbeitsschritt).split()[0]  # Ersten Wert extrahieren\n",
    "    print(f\"Arbeitsschritt extrahierte Nummer: {step_number}\")\n",
    "\n",
    "    # Trennen von Features und Zielvariable\n",
    "    target_column = 'user_of_latest_model'\n",
    "    X_test = df.drop(columns=[target_column])\n",
    "    y_test = df[target_column]\n",
    "\n",
    "    # Identifikation der kategorischen Variablen\n",
    "    categorical_cols = X_test.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "   # One-Hot-Encoding f√ºr kategoriale Variablen\n",
    "    encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_encoded = pd.DataFrame(\n",
    "        encoder.fit_transform(X[categorical_cols]),\n",
    "        columns=encoder.get_feature_names_out(categorical_cols),\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "        # Numerische Spalten beibehalten\n",
    "        X_numeric = X_test.drop(columns=categorical_cols)\n",
    "\n",
    "        # Zusammenf√ºgen der numerischen und encodierten Daten\n",
    "        X_final = pd.concat([X_numeric, X_encoded], axis=1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEHLER: Encoder-Datei {encoder_filename} nicht gefunden! √úberspringe OneHotEncoding.\")\n",
    "        X_final = X_test  # Falls kein Encoding n√∂tig\n",
    "\n",
    "    # **Laden des trainierten Modells**\n",
    "    model_filename = f\"model_parameters_Andreas_{step_number}.pkl\"\n",
    "    try:\n",
    "        model = joblib.load(model_filename)\n",
    "        print(f\"Geladenes Modell: {model_filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEHLER: Modell-Datei {model_filename} nicht gefunden!\")\n",
    "        return None\n",
    "\n",
    "    # **Vorhersagen auf dem Testdatensatz**\n",
    "    y_pred = model.predict(X_final)\n",
    "\n",
    "    # **Metriken berechnen**\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Testset - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}\")\n",
    "\n",
    "    # Ergebnisse in DataFrame speichern\n",
    "    new_row = pd.DataFrame({\n",
    "        \"arbeitsschritt\": [arbeitsschritt],\n",
    "        \"Test_Accuracy\": [accuracy],\n",
    "        \"Test_F1-Score\": [f1]\n",
    "    })\n",
    "    df_score_test = pd.concat([df_score_test, new_row], ignore_index=True)\n",
    "\n",
    "    return df_score_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step0, '0 Simple Preprocessing')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Data Quality Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step1 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úberblick √ºber das Feature \"ctry\"\n",
    "ctry_overview = df_step1['ctry'].value_counts()\n",
    "# Ergebnis anzeigen\n",
    "print(ctry_overview)\n",
    "\n",
    "# Korrigiere den L√§ndernamen \"Germayn\" zu \"Germany\"\n",
    "df_step1['ctry'] = df_step1['ctry'].replace('Germayn', 'Germany')\n",
    "\n",
    "# √úberpr√ºfen, ob die √Ñnderung erfolgreich war\n",
    "ctry_overview = df_step1['ctry'].value_counts()\n",
    "print(ctry_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doppelte Datens√§tze anzeigen\n",
    "duplicate_rows = df_step1[df_step1.duplicated()]\n",
    "print(\"Doppelte Datens√§tze:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Doppelte Datens√§tze entfernen\n",
    "df_step1 = df_step1.drop_duplicates()\n",
    "print(\"Doppelte Datens√§tze wurden entfernt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step1, '1 Data Quality Correction')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listwise deletion all rows with missing values in the column 'user_of_latest_model'\n",
    "df_step2=df_step2.drop(df_step2[df_step2['user_of_latest_model'].isnull()].index)\n",
    "msno.matrix(df_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step2[numeric].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_A = df_step2[numeric].corr()['biking_hours_per_week'].drop('biking_hours_per_week')  # Entferne die Korrelation mit sich selbst\n",
    "\n",
    "# Ausgabe der Korrelation\n",
    "print(correlation_with_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Iterative Imputation for numeric columns\n",
    "imputer = IterativeImputer(max_iter=30, tol=1e-2, random_state=42, initial_strategy='median') #initial all missing values were replaced by median\n",
    "df_step2_numeric_imputed = pd.DataFrame(imputer.fit_transform(df_step2[numeric])) # creating a new dataframe with imputed values\n",
    "\n",
    "df_step2_numeric_imputed.index = df_step2.index # adapt index of new dataframe to index of original dataframe\n",
    "df_step2[numeric] = df_step2_numeric_imputed # replace numeric columns in original dataframe with imputed values\n",
    "\n",
    "print(df_step2[numeric].isnull().sum()) # check if all missing values in numeric columns were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Kategorische Spalten in numerische Werte umwandeln\n",
    "label_encoders = {}  # Dictionary, um die LabelEncoder zu speichern\n",
    "\n",
    "for col in categorical:\n",
    "    if df_step2[col].dtype == 'object':  # √úberpr√ºfen, ob es sich um eine kategorische Spalte handelt\n",
    "        encoder = LabelEncoder()\n",
    "        df_step2[col] = encoder.fit_transform(df_step2[col].fillna('Missing'))  # Umwandlung und fehlende Werte ersetzen\n",
    "        label_encoders[col] = encoder  # Speichern des Encoders\n",
    "\n",
    "# 2. KNN-Imputation auf den numerischen Werten anwenden\n",
    "imputer = KNNImputer(n_neighbors=2)  # Anzahl der Nachbarn (k) einstellen\n",
    "df_imputed_categorical = pd.DataFrame(imputer.fit_transform(df_step2[categorical]), columns=categorical)\n",
    "\n",
    "# 3. Imputierte Werte zur√ºck in kategorische Werte umwandeln\n",
    "for col, encoder in label_encoders.items():\n",
    "    df_imputed_categorical[col] = encoder.inverse_transform(df_imputed_categorical[col].round().astype(int))  # R√ºckumwandlung\n",
    "\n",
    "# Ausgabe des imputierten DataFrames\n",
    "df_imputed_categorical\n",
    "\n",
    "df_imputed_categorical.index = df_step2.index # adapt index of new dataframe to index of original dataframe\n",
    "df_step2[categorical] = df_imputed_categorical # replace numeric columns in original dataframe with imputed values\n",
    "\n",
    "print(df_step2.isnull().sum()) # check if all missing values in numeric columns were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step2, '2 Missing Value Handling')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step3 = df_step0.copy()\n",
    "\n",
    "#  Detection of outliers with IQR-method\n",
    "def detect_outliers_iqr(df):\n",
    "    df_outliers = df.copy()\n",
    "    for col in df.select_dtypes(include=np.number):  # Nur numerische Spalten\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_outliers[col] = df[col].apply(lambda x: np.nan if x < lower_bound or x > upper_bound else x)\n",
    "    return df_outliers\n",
    "\n",
    "\n",
    "#  Replacement of outliers mit NaN\n",
    "data_numeric_no_outliers = detect_outliers_iqr(df_step3[numeric])\n",
    "\n",
    "data_numeric_no_outliers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Iterative Imputation for numeric columns\n",
    "imputer = IterativeImputer(max_iter=30, tol=1e-2, random_state=42, initial_strategy='median') #initial all missing values were replaced by median\n",
    "data_numeric_imputed = pd.DataFrame(imputer.fit_transform(data_numeric_no_outliers)) # creating a new dataframe with imputed values\n",
    "\n",
    "data_numeric_imputed.index = df_step3.index # adapt index of new dataframe to index of original dataframe\n",
    "#df_imputed_numeric = df_step3.copy() # create a copy of the original dataframe\n",
    "df_step3[numeric] = data_numeric_imputed # replace numeric columns in original dataframe with imputed values\n",
    "\n",
    "print(df_step3[numeric].isnull().sum()) # check if all missing values in numeric columns were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step3, '3 Outliers')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Transformation Normalverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step4 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the distribution of the numerical data with histograms\n",
    "%matplotlib inline\n",
    "hist = df_step4[numeric].hist(bins=30,figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_ohne_target = numeric.copy()\n",
    "numeric_ohne_target.remove('user_of_latest_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Hauptzweck dieser Transformation ist:\n",
    "Umwandlung nicht-normalverteilter Daten in normalverteilte Daten\n",
    "Reduzierung des Einflusses von Ausrei√üern\n",
    "Verbesserung der Performance von Machine Learning Modellen, die von normalverteilten Daten profitieren\n",
    "Standardisierung der Datenverteilung √ºber alle numerischen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "#Erstellt einen Transformer, der die Daten in eine Normalverteilung √ºberf√ºhrt\n",
    "qt = QuantileTransformer(n_quantiles=25, output_distribution='normal', random_state=0)\n",
    "\n",
    "#Lernt die Transformation aus den Daten und wendet sie direkt an\n",
    "#Transformiert alle numerischen Spalten au√üer 'user_of_latest_model'\n",
    "#Das Ergebnis ist eine NumPy-Array mit den transformierten Werten\n",
    "trans_x = qt.fit_transform(df_step4[numeric_ohne_target])  \n",
    "\n",
    "#Wandelt die transformierte Array wieder in einen DataFrame um\n",
    "#Ersetzt die urspr√ºnglichen Werte im DataFrame mit den transformierten Werten\n",
    "#Beh√§lt die Index-Struktur bei\n",
    "df_step4[numeric_ohne_target] = pd.DataFrame(trans_x, columns=numeric_ohne_target, index=df_step4.index)\n",
    "\n",
    "# Plot histograms for each numerical column to visualize the distribution\n",
    "df_step4[numeric].hist(bins=20, figsize=(10, 10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step4, '4 Transformation Normalverteilung')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Power Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step5 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize PowerTransformer (Yeo-Johnson is default, Box-Cox requires only positive data)\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)  \n",
    "\n",
    "# Apply the transformation to all numerical columns\n",
    "trans_x = pt.fit_transform(df_step5[numeric_ohne_target])  \n",
    "\n",
    "# Convert the transformed array back to a DataFrame and replace original numerical columns\n",
    "df_step5[numeric_ohne_target] = pd.DataFrame(trans_x, columns=numeric_ohne_target, index=df_step5.index)\n",
    "\n",
    "# Plot histograms for each numerical column to visualize the distribution\n",
    "df_step5[numeric].hist(bins=20, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step5, '5 Power Transformation')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Min-Max-Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step6 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialisiere den MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Transformiere die numerischen Spalten (ohne das Label)\n",
    "scaled_features = scaler.fit_transform(df_step6[numeric_ohne_target])\n",
    "\n",
    "# Konvertiere die transformierten Daten zur√ºck in einen DataFrame mit den urspr√ºnglichen Spaltennamen\n",
    "df_step6[numeric_ohne_target] = pd.DataFrame(scaled_features, \n",
    "                                            columns=numeric_ohne_target, \n",
    "                                            index=df_step6.index)\n",
    "\n",
    "# Visualisiere die Verteilung der skalierten Daten\n",
    "df_step6[numeric].hist(bins=20, figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "# Optional: √úberpr√ºfung der Skalierung\n",
    "print(\"\\nMin-Max Werte nach der Skalierung:\")\n",
    "for column in numeric_ohne_target:\n",
    "    print(f\"{column}:\")\n",
    "    print(f\"Min: {df_step6[column].min():.2f}\")\n",
    "    print(f\"Max: {df_step6[column].max():.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step6, '6 MIN-MAX-Scaler')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Standard-Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step7 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler transformiert die Daten so, dass:\n",
    "Mittelwert = 0\n",
    "Standardabweichung = 1\n",
    "Die Werte sind nicht auf einen bestimmten Bereich beschr√§nkt\n",
    "Die Transformation erfolgt nach der Formel: z = (x - Œº) / œÉ\n",
    "\n",
    "Erstellt eine Kopie von df_step4\n",
    "F√ºhrt die Standardisierung der numerischen Features durch\n",
    "Beh√§lt die Zielvariable unver√§ndert\n",
    "Visualisiert die Verteilungen\n",
    "√úberpr√ºft die erfolgreiche Standardisierung durch Ausgabe von Mittelwert und Standardabweichung\n",
    "\n",
    "Die standardisierten Werte sollten nun:\n",
    "\n",
    "Einen Mittelwert nahe 0 haben\n",
    "Eine Standardabweichung nahe 1 haben\n",
    "Die urspr√ºngliche Verteilungsform beibehalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere den StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transformiere die numerischen Spalten (ohne das Label)\n",
    "scaled_features = scaler.fit_transform(df_step7[numeric_ohne_target])\n",
    "\n",
    "# Konvertiere die transformierten Daten zur√ºck in einen DataFrame\n",
    "df_step7[numeric_ohne_target] = pd.DataFrame(scaled_features, \n",
    "                                            columns=numeric_ohne_target, \n",
    "                                            index=df_step7.index)\n",
    "\n",
    "# Visualisiere die Verteilung\n",
    "plt.figure(figsize=(10, 10))\n",
    "df_step7[numeric].hist(bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# √úberpr√ºfe die Standardisierung\n",
    "print(\"\\n√úberpr√ºfung der Standardisierung (Mean ‚âà 0, Std ‚âà 1):\")\n",
    "for column in numeric_ohne_target:\n",
    "    print(f\"\\n{column}:\")\n",
    "    print(f\"Mittelwert: {df_step7[column].mean():.4f}\")\n",
    "    print(f\"Standardabweichung: {df_step7[column].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step7, '7 Standard-Scaler')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step8 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Ordinal Encoding werden kategorische Variablen in numerische Werte umgewandelt, wobei jeder Kategorie eine eindeutige Zahl zugeordnet wird. Dies ist besonders sinnvoll, wenn die Kategorien eine nat√ºrliche Ordnung oder Rangfolge haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle kategorischen Spalten im DataFrame anzeigen\n",
    "categorical_columns = df_step8.select_dtypes(include=['object']).columns\n",
    "print(\"Kategorische Spalten im DataFrame:\")\n",
    "print(categorical_columns.tolist())\n",
    "\n",
    "# F√ºr jede kategorische Spalte die unique Werte anzeigen\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nUnique Werte in {col}:\")\n",
    "    print(df_step8[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# ‚úÖ Spalten mit nat√ºrlicher Ordnung f√ºr Ordinal Encoding\n",
    "ordinal_columns = ['subscription_type', 'goal_of_training', 'preferred_training_daytime']\n",
    "\n",
    "# Mapping f√ºr subscription_type (manuelles Ordinal Encoding)\n",
    "subscription_mapping = {'Free': 0, 'Basic': 1, 'Premium': 2}\n",
    "df_step8['subscription_type'] = df_step8['subscription_type'].map(subscription_mapping)\n",
    "\n",
    "# ‚úÖ Manuelle Reihenfolge f√ºr OrdinalEncoder\n",
    "ordinal_mappings = [\n",
    "    ['Recreation', 'Fitness', 'Competition'],  # Reihenfolge f√ºr goal_of_training\n",
    "    ['Morning', 'Afternoon', 'Evening']  # Reihenfolge f√ºr preferred_training_daytime\n",
    "]\n",
    "\n",
    "# Ordinal Encoding anwenden\n",
    "ordinal_encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "df_step8[['goal_of_training', 'preferred_training_daytime']] = ordinal_encoder.fit_transform(\n",
    "    df_step8[['goal_of_training', 'preferred_training_daytime']]\n",
    ")\n",
    "\n",
    "# üîπ Ausgabe der Zuordnungen\n",
    "print(\"\\nZuordnungen f√ºr goal_of_training & preferred_training_daytime:\")\n",
    "for i, column in enumerate(['goal_of_training', 'preferred_training_daytime']):\n",
    "    print(f\"\\n{column}:\")\n",
    "    for j, category in enumerate(ordinal_encoder.categories_[i]):\n",
    "        print(f\"{category} -> {j}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step8, '8 Ordinal Encoding')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9 Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step9 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F√ºhrt ANOVA F-Test durch, um die statistische Signifikanz der Features zu bewerten. \n",
    "Nutzt Random Forest Feature Importance f√ºr eine modellbasierte Bewertung.\n",
    "Erstellt eine Korrelationsmatrix zur Identifizierung redundanter Features.\n",
    "Kombiniert die Ergebnisse zu einer Empfehlung.\n",
    "Beh√§lt nur die wichtigsten Features im finalen DataFrame.\n",
    "Die Entscheidung, welche Features behalten werden sollen, basiert auf:\n",
    "* Hoher Feature Importance\n",
    "* Statistischer Signifikanz (niedriger p-Wert im ANOVA Test)\n",
    "* Geringer Korrelation mit anderen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Features und Target definieren\n",
    "X = df_step9[numeric_ohne_target]  # Numerische Features ohne Label\n",
    "y = df_step9['user_of_latest_model']  # Zielvariable\n",
    "\n",
    "# 1. ANOVA F-Test f√ºr numerische Features\n",
    "f_selector = SelectKBest(f_classif, k='all')\n",
    "f_selector.fit(X, y)\n",
    "\n",
    "# Feature Scores aus ANOVA F-Test\n",
    "anova_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F_Score': f_selector.scores_,\n",
    "    'P_value': f_selector.pvalues_\n",
    "})\n",
    "anova_scores = anova_scores.sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(\"ANOVA F-Test Scores:\")\n",
    "print(anova_scores)\n",
    "\n",
    "# 2. Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature Importance aus Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "rf_importance = rf_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(rf_importance)\n",
    "\n",
    "# 3. Korrelationsmatrix f√ºr numerische Features\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Identifiziere stark korrelierte Features (z.B. > 0.8)\n",
    "high_correlation = np.where(np.abs(correlation_matrix) > 0.8)\n",
    "high_correlation = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                   for x, y in zip(*high_correlation) if x != y and x < y]\n",
    "\n",
    "print(\"\\nStark korrelierte Features (>0.8):\")\n",
    "for feat1, feat2, corr in high_correlation:\n",
    "    print(f\"{feat1} - {feat2}: {corr:.2f}\")\n",
    "\n",
    "# Kombiniere die Ergebnisse f√ºr eine Empfehlung\n",
    "# W√§hle Features basierend auf Importance und geringer Korrelation\n",
    "important_features = rf_importance[rf_importance['Importance'] > rf_importance['Importance'].mean()]['Feature'].tolist()\n",
    "\n",
    "print(\"\\nEmpfohlene Features basierend auf Importance und geringer Korrelation:\")\n",
    "print(important_features)\n",
    "\n",
    "# Optional: Erstelle einen neuen DataFrame nur mit den wichtigsten Features\n",
    "selected_features = important_features + ['user_of_latest_model']  # F√ºge Label hinzu\n",
    "df_step9 = df_step9[selected_features]\n",
    "\n",
    "# Visualisiere die Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(rf_importance['Feature'], rf_importance['Importance'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Feature Importance aus Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nun die Feature Selection der kategorischen Variablen:\n",
    "Nutzt die bereits vorhandenen kategorischen Spalten (categorical_columns)\n",
    "F√ºhrt das Ordinal Encoding durch\n",
    "Berechnet die Feature Importance f√ºr kategorische Features\n",
    "Visualisiert die Ergebnisse\n",
    "W√§hlt wichtige kategorische Features aus\n",
    "Kombiniert sie mit den bereits ausgew√§hlten numerischen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zur√ºck zum urspr√ºnglichen DataFrame f√ºr die kategorische Feature Selection, da im vorherigen Schritt die numerischen ausgesondert wurden\n",
    "df_temp = df_step0.copy()\n",
    "\n",
    "# Encoding f√ºr kategorische Features\n",
    "encoder = OrdinalEncoder()\n",
    "X_cat = pd.DataFrame(encoder.fit_transform(df_temp[categorical_columns]), \n",
    "                    columns=categorical_columns)\n",
    "y_cat = df_temp['user_of_latest_model']\n",
    "\n",
    "# Random Forest f√ºr kategorische Features\n",
    "rf_cat = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_cat.fit(X_cat, y_cat)\n",
    "\n",
    "# Feature Importance f√ºr kategorische Features\n",
    "rf_importance_cat = pd.DataFrame({\n",
    "    'Feature': X_cat.columns,\n",
    "    'Importance': rf_cat.feature_importances_\n",
    "})\n",
    "rf_importance_cat = rf_importance_cat.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance (kategorische Features):\")\n",
    "print(rf_importance_cat)\n",
    "\n",
    "# Wichtige kategorische Features ausw√§hlen\n",
    "important_cat_features = rf_importance_cat[rf_importance_cat['Importance'] > \n",
    "                                         rf_importance_cat['Importance'].mean()]['Feature'].tolist()\n",
    "\n",
    "print(\"\\nWichtige kategorische Features:\")\n",
    "print(important_cat_features)\n",
    "\n",
    "# Visualisierung der kategorischen Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(rf_importance_cat['Feature'], rf_importance_cat['Importance'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Feature Importance - Kategorische Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Finalen DataFrame aktualisieren mit numerischen und kategorischen Features\n",
    "all_selected_features = selected_features + important_cat_features\n",
    "df_step9 = df_temp[all_selected_features]\n",
    "\n",
    "print(\"\\nFinale ausgew√§hlte Features (numerisch + kategorisch):\")\n",
    "print(df_step9.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step9, '9 Feature Selection')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisiert die numerischen Daten\n",
    "F√ºhrt PCA durch\n",
    "Analysiert die erkl√§rte Varianz\n",
    "Visualisiert die kumulierte erkl√§rte Varianz\n",
    "W√§hlt die optimale Anzahl von Komponenten\n",
    "Erstellt einen neuen DataFrame mit:\n",
    "\n",
    "PCA-Komponenten f√ºr numerische Features\n",
    "Originalen kategorischen Features\n",
    "Target-Variable\n",
    "\n",
    "Schwellenwert f√ºr die erkl√§rte Varianz (hier 90%) nach Bedarf anpassen.\n",
    "Die Feature Loadings am Ende zeigen, welche originalen Features am wichtigsten f√ºr jede Hauptkomponente sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step10 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1Ô∏è‚É£ Numerische Werte aus df_step10 extrahieren\n",
    "X = df_step10.select_dtypes(include=['int64', 'float64']).drop(columns=['user_of_latest_model'], errors='ignore')\n",
    "y = df_step10['user_of_latest_model']\n",
    "\n",
    "# 2Ô∏è‚É£ Standardisierung der numerischen Daten (wichtig f√ºr PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3Ô∏è‚É£ PCA anwenden (hier mit 2 Hauptkomponenten, kann angepasst werden)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 4Ô∏è‚É£ PCA-Ergebnis in einen neuen DataFrame umwandeln\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df_step10.index)\n",
    "\n",
    "# 5Ô∏è‚É£ Zielvariable wieder hinzuf√ºgen\n",
    "df_pca['user_of_latest_model'] = y.values  \n",
    "\n",
    "# 6Ô∏è‚É£ Ergebnis anzeigen\n",
    "print(\"Erkl√§rte Varianz:\", pca.explained_variance_ratio_)\n",
    "df_pca\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_pca, '10 Feature Extraction')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung der Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib f√ºr bessere Lesbarkeit konfigurieren\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Zwei Linien plotten\n",
    "plt.plot(df_score_test.index, df_score_test['Accuracy'], marker='o', label='Accuracy', color='blue')\n",
    "plt.plot(df_score_test.index, df_score_test['F1-Score'], marker='o', label='F1-Score', color='red')\n",
    "\n",
    "# Beschriftungen und Titel\n",
    "plt.xlabel('Preprocessing Schritte')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Entwicklung von Accuracy und F1-Score √ºber die Preprocessing Schritte')\n",
    "\n",
    "# Legende hinzuf√ºgen\n",
    "plt.legend()\n",
    "\n",
    "# Grid f√ºr bessere Lesbarkeit\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# X-Achsen-Labels rotieren f√ºr bessere Lesbarkeit\n",
    "plt.xticks(df_score_test.index, ['Step '+str(i) for i in df_score_test.index], rotation=45)\n",
    "\n",
    "# Layout optimieren\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
