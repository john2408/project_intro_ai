{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Step 0 bis Step 7 + Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import missingno as msno\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of         id  age_of_customer     sex       ctry         town  \\\n",
       "0    C0001             60.0   Other    Germany      Hamburg   \n",
       "1    C0002             53.0    Male    Germayn       Berlin   \n",
       "2    C0003             30.0   Other         UK       London   \n",
       "3    C0004             24.0   Other    Germany      Hamburg   \n",
       "4    C0005             53.0    Male        USA  Los Angeles   \n",
       "..     ...              ...     ...        ...          ...   \n",
       "495  C0495             43.0  Female         UK   Birmingham   \n",
       "496  C0496             60.0  Female        USA      Chicago   \n",
       "497  C0498             44.0    Male      India        Delhi   \n",
       "498  C0499             46.0  Female         UK       London   \n",
       "499  C0500             64.0   Other  Australia     Brisbane   \n",
       "\n",
       "     swimming_hours_per_week  biking_hours_per_week  running_hours_per_week  \\\n",
       "0                       1.80                   4.44                    0.34   \n",
       "1                       1.77                   5.42                    5.60   \n",
       "2                       4.05                   6.98                    4.03   \n",
       "3                       3.22                  10.54                    4.23   \n",
       "4                       3.15                   3.03                    5.40   \n",
       "..                       ...                    ...                     ...   \n",
       "495                     4.55                   0.86                    5.19   \n",
       "496                     1.18                  10.67                    7.19   \n",
       "497                     1.07                   3.80                    2.87   \n",
       "498                     0.00                   8.81                    6.31   \n",
       "499                     2.86                   1.08                    8.19   \n",
       "\n",
       "     total_training_hours_per_week  vo2_max  ...  calories_burned_per_week  \\\n",
       "0                             6.58    23.02  ...                   3062.83   \n",
       "1                            12.79    52.46  ...                   6651.29   \n",
       "2                            15.07    73.21  ...                   7506.12   \n",
       "3                            17.99    74.64  ...                   9134.26   \n",
       "4                            11.58    51.22  ...                   5709.64   \n",
       "..                             ...      ...  ...                       ...   \n",
       "495                          10.60    59.16  ...                   5579.17   \n",
       "496                          19.04    44.23  ...                   9713.87   \n",
       "497                           7.73    55.21  ...                   3679.10   \n",
       "498                          15.12    55.85  ...                   7337.50   \n",
       "499                          12.14      NaN  ...                   5850.79   \n",
       "\n",
       "     support_cases_of_customer  customer_years  most_current_software_update  \\\n",
       "0                            3               4                    2023-12-05   \n",
       "1                            2               4                    2024-04-23   \n",
       "2                            0               6                    2024-12-30   \n",
       "3                            3               6                    2023-10-30   \n",
       "4                            3               8                    2024-12-23   \n",
       "..                         ...             ...                           ...   \n",
       "495                          2               3                    2024-06-09   \n",
       "496                          4               5                    2024-07-20   \n",
       "497                          2               1                    2024-04-04   \n",
       "498                          1               1                    2024-05-15   \n",
       "499                          2               4                    2024-10-26   \n",
       "\n",
       "    goal_of_training preferred_training_daytime subscription_type  \\\n",
       "0         Recreation                    Evening           Premium   \n",
       "1         Recreation                    Evening              Free   \n",
       "2        Competition                    Morning              Free   \n",
       "3        Competition                    Morning           Premium   \n",
       "4            Fitness                  Afternoon              Free   \n",
       "..               ...                        ...               ...   \n",
       "495          Fitness                  Afternoon           Premium   \n",
       "496      Competition                    Morning             Basic   \n",
       "497          Fitness                    Morning              Free   \n",
       "498          Fitness                    Morning             Basic   \n",
       "499          Fitness                  Afternoon             Basic   \n",
       "\n",
       "    color_of_watch synchronisation user_of_latest_model  \n",
       "0            Black              No                    0  \n",
       "1            White             Yes                    0  \n",
       "2            Black             Yes                    1  \n",
       "3            White              No                    1  \n",
       "4            Black             Yes                    0  \n",
       "..             ...             ...                  ...  \n",
       "495          Black              No                    1  \n",
       "496          White             Yes                    0  \n",
       "497          Black             Yes                    0  \n",
       "498          Black              No                    0  \n",
       "499          Black             Yes                    0  \n",
       "\n",
       "[500 rows x 21 columns]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/raw/triathlon_watch_test_data_final.csv')\n",
    "# Rename columns for better handling\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spalte ID wird das führende \"C\" entfernt und die führenden Nullen\n",
    "df[\"id\"] = df[\"id\"].dropna().str.replace(\"C\", \"\", regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  most_current_software_update  days_since_latest_update\n",
      "0                   2023-12-05                       391\n",
      "1                   2024-04-23                       251\n",
      "2                   2024-12-30                         0\n",
      "3                   2023-10-30                       427\n",
      "4                   2024-12-23                         7\n"
     ]
    }
   ],
   "source": [
    "# Sicherstellen, dass die Spalte als Datumsformat erkannt wird\n",
    "df[\"most_current_software_update\"] = pd.to_datetime(df[\"most_current_software_update\"], format=\"%Y-%m-%d\")\n",
    "# ✅ Differenz zum jüngsten Datum berechnen (in Tagen)\n",
    "latest_date = df[\"most_current_software_update\"].max()\n",
    "df[\"days_since_latest_update\"] = (latest_date - df[\"most_current_software_update\"]).dt.days\n",
    "# Ergebnis ausgeben\n",
    "print(df[[\"most_current_software_update\", \"days_since_latest_update\"]].head())\n",
    "df = df.drop(columns=[\"most_current_software_update\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric=df.select_dtypes(include=['float64']).columns.tolist() #list of numeric columns\n",
    "categorical = df.select_dtypes(include=['object']).columns.to_list() #append all categorical columns to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                               float64\n",
      "age_of_customer                  float64\n",
      "sex                               object\n",
      "ctry                              object\n",
      "town                              object\n",
      "swimming_hours_per_week          float64\n",
      "biking_hours_per_week            float64\n",
      "running_hours_per_week           float64\n",
      "total_training_hours_per_week    float64\n",
      "vo2_max                          float64\n",
      "10k_running_time_prediction      float64\n",
      "calories_burned_per_week         float64\n",
      "support_cases_of_customer          int64\n",
      "customer_years                     int64\n",
      "goal_of_training                  object\n",
      "preferred_training_daytime        object\n",
      "subscription_type                 object\n",
      "color_of_watch                    object\n",
      "synchronisation                   object\n",
      "user_of_latest_model               int64\n",
      "days_since_latest_update           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bis hierin wurde nur ganz einfache Veränderungen vollzogen. df ist nun die Grundlage für die weietren Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step0 =df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_column = \"user_of_latest_model\" if \"user_of_latest_model\" in df_step0.columns else df.columns[-1]\n",
    "# Zeilen mit fehlenden Werten in der Zielvariable entfernen\n",
    "df_step0 = df_step0.dropna(subset=[target_column])\n",
    "# Zeilen mit mehr als zwei fehlenden Werten entfernen\n",
    "df_step0 = df_step0[df_step0.isnull().sum(axis=1) <= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 abgeschlossen. Bereinigte Daten gespeichert unter: triathlon_watch_training_data_step0.csv\n",
      "    id  age_of_customer    sex     ctry         town  swimming_hours_per_week  \\\n",
      "0  1.0             60.0  Other  Germany      Hamburg                     1.80   \n",
      "1  2.0             53.0   Male  Germayn       Berlin                     1.77   \n",
      "2  3.0             30.0  Other       UK       London                     4.05   \n",
      "3  4.0             24.0  Other  Germany      Hamburg                     3.22   \n",
      "4  5.0             53.0   Male      USA  Los Angeles                     3.15   \n",
      "\n",
      "   biking_hours_per_week  running_hours_per_week  \\\n",
      "0                   4.44                    0.34   \n",
      "1                   5.42                    5.60   \n",
      "2                   6.98                    4.03   \n",
      "3                  10.54                    4.23   \n",
      "4                   3.03                    5.40   \n",
      "\n",
      "   total_training_hours_per_week  vo2_max  ...  calories_burned_per_week  \\\n",
      "0                           6.58    23.02  ...                   3062.83   \n",
      "1                          12.79    52.46  ...                   6651.29   \n",
      "2                          15.07    73.21  ...                   7506.12   \n",
      "3                          17.99    74.64  ...                   9134.26   \n",
      "4                          11.58    51.22  ...                   5709.64   \n",
      "\n",
      "   support_cases_of_customer  customer_years  goal_of_training  \\\n",
      "0                          3               4        Recreation   \n",
      "1                          2               4        Recreation   \n",
      "2                          0               6       Competition   \n",
      "3                          3               6       Competition   \n",
      "4                          3               8           Fitness   \n",
      "\n",
      "  preferred_training_daytime subscription_type color_of_watch synchronisation  \\\n",
      "0                    Evening           Premium          Black              No   \n",
      "1                    Evening              Free          White             Yes   \n",
      "2                    Morning              Free          Black             Yes   \n",
      "3                    Morning           Premium          White              No   \n",
      "4                  Afternoon              Free          Black             Yes   \n",
      "\n",
      "  user_of_latest_model  days_since_latest_update  \n",
      "0                    0                       391  \n",
      "1                    0                       251  \n",
      "2                    1                         0  \n",
      "3                    1                       427  \n",
      "4                    0                         7  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fehlende Werte analysieren\n",
    "missing_values = df_step0.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "# Fehlende Werte durch Durchschnitt (numerisch) oder Modus (kategorisch) ersetzen\n",
    "#df_step0 = df_step0.copy()\n",
    "\n",
    "numeric_cols = df_step0.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df_step0.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_step0[col] = df_step0[col].fillna(df_step0[col].mean())\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_step0[col] = df_step0[col].fillna(df_step0[col].mode()[0])\n",
    "\n",
    "# Speichern der bereinigten Daten\n",
    "output_path = \"triathlon_watch_training_data_step0.csv\"\n",
    "df_step0.to_csv(output_path, index=False)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(\"Step 0 abgeschlossen. Bereinigte Daten gespeichert unter:\", output_path)\n",
    "print(df_step0.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Numerische Spalten: ['id', 'age_of_customer', 'swimming_hours_per_week', 'biking_hours_per_week', 'running_hours_per_week', 'total_training_hours_per_week', 'vo2_max', '10k_running_time_prediction', 'calories_burned_per_week', 'support_cases_of_customer', 'customer_years', 'user_of_latest_model', 'days_since_latest_update']\n",
      "🔠 Kategoriale Spalten: ['sex', 'ctry', 'town', 'goal_of_training', 'preferred_training_daytime', 'subscription_type', 'color_of_watch', 'synchronisation']\n"
     ]
    }
   ],
   "source": [
    "numerical_columns = df_step0.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_columns = df_step0.select_dtypes(exclude=['number']).columns.tolist()\n",
    "\n",
    "print(\"📊 Numerische Spalten:\", numerical_columns)\n",
    "print(\"🔠 Kategoriale Spalten:\", categorical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression als Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_test = pd.DataFrame(columns=['arbeitsschritt', 'Accuracy', 'F1-Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def run_logistic_regression(df, arbeitsschritt):\n",
    "    global df_score_test\n",
    "\n",
    "    # **Extrahiere die erste Zahl aus arbeitsschritt**\n",
    "    step_number = str(arbeitsschritt).split()[0]  # Ersten Wert extrahieren\n",
    "    print(f\"Arbeitsschritt extrahierte Nummer: {step_number}\")\n",
    "\n",
    "    # Trennen von Features und Zielvariable\n",
    "    target_column = 'user_of_latest_model'\n",
    "    X_test = df.drop(columns=[target_column])\n",
    "    y_test = df[target_column]\n",
    "\n",
    "    # Identifikation der kategorischen Variablen\n",
    "    categorical_cols = X_test.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "   # One-Hot-Encoding für kategoriale Variablen\n",
    "    encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_encoded = pd.DataFrame(\n",
    "        encoder.fit_transform(X[categorical_cols]),\n",
    "        columns=encoder.get_feature_names_out(categorical_cols),\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "        # Numerische Spalten beibehalten\n",
    "        X_numeric = X_test.drop(columns=categorical_cols)\n",
    "\n",
    "        # Zusammenfügen der numerischen und encodierten Daten\n",
    "        X_final = pd.concat([X_numeric, X_encoded], axis=1)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEHLER: Encoder-Datei {encoder_filename} nicht gefunden! Überspringe OneHotEncoding.\")\n",
    "        X_final = X_test  # Falls kein Encoding nötig\n",
    "\n",
    "    # **Laden des trainierten Modells**\n",
    "    model_filename = f\"model_parameters_Andreas_{step_number}.pkl\"\n",
    "    try:\n",
    "        model = joblib.load(model_filename)\n",
    "        print(f\"Geladenes Modell: {model_filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FEHLER: Modell-Datei {model_filename} nicht gefunden!\")\n",
    "        return None\n",
    "\n",
    "    # **Vorhersagen auf dem Testdatensatz**\n",
    "    y_pred = model.predict(X_final)\n",
    "\n",
    "    # **Metriken berechnen**\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Testset - Accuracy: {accuracy:.2f}, F1-Score: {f1:.2f}\")\n",
    "\n",
    "    # Ergebnisse in DataFrame speichern\n",
    "    new_row = pd.DataFrame({\n",
    "        \"arbeitsschritt\": [arbeitsschritt],\n",
    "        \"Test_Accuracy\": [accuracy],\n",
    "        \"Test_F1-Score\": [f1]\n",
    "    })\n",
    "    df_score_test = pd.concat([df_score_test, new_row], ignore_index=True)\n",
    "\n",
    "    return df_score_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step0, '0 Simple Preprocessing')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Data Quality Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step1 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überblick über das Feature \"ctry\"\n",
    "ctry_overview = df_step1['ctry'].value_counts()\n",
    "# Ergebnis anzeigen\n",
    "print(ctry_overview)\n",
    "\n",
    "# Korrigiere den Ländernamen \"Germayn\" zu \"Germany\"\n",
    "df_step1['ctry'] = df_step1['ctry'].replace('Germayn', 'Germany')\n",
    "\n",
    "# Überprüfen, ob die Änderung erfolgreich war\n",
    "ctry_overview = df_step1['ctry'].value_counts()\n",
    "print(ctry_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doppelte Datensätze anzeigen\n",
    "duplicate_rows = df_step1[df_step1.duplicated()]\n",
    "print(\"Doppelte Datensätze:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Doppelte Datensätze entfernen\n",
    "df_step1 = df_step1.drop_duplicates()\n",
    "print(\"Doppelte Datensätze wurden entfernt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step1, '1 Data Quality Correction')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listwise deletion all rows with missing values in the column 'user_of_latest_model'\n",
    "df_step2=df_step2.drop(df_step2[df_step2['user_of_latest_model'].isnull()].index)\n",
    "msno.matrix(df_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step2[numeric].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_A = df_step2[numeric].corr()['biking_hours_per_week'].drop('biking_hours_per_week')  # Entferne die Korrelation mit sich selbst\n",
    "\n",
    "# Ausgabe der Korrelation\n",
    "print(correlation_with_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Iterative Imputation for numeric columns\n",
    "imputer = IterativeImputer(max_iter=30, tol=1e-2, random_state=42, initial_strategy='median') #initial all missing values were replaced by median\n",
    "df_step2_numeric_imputed = pd.DataFrame(imputer.fit_transform(df_step2[numeric])) # creating a new dataframe with imputed values\n",
    "\n",
    "df_step2_numeric_imputed.index = df_step2.index # adapt index of new dataframe to index of original dataframe\n",
    "df_step2[numeric] = df_step2_numeric_imputed # replace numeric columns in original dataframe with imputed values\n",
    "\n",
    "print(df_step2[numeric].isnull().sum()) # check if all missing values in numeric columns were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Kategorische Spalten in numerische Werte umwandeln\n",
    "label_encoders = {}  # Dictionary, um die LabelEncoder zu speichern\n",
    "\n",
    "for col in categorical:\n",
    "    if df_step2[col].dtype == 'object':  # Überprüfen, ob es sich um eine kategorische Spalte handelt\n",
    "        encoder = LabelEncoder()\n",
    "        df_step2[col] = encoder.fit_transform(df_step2[col].fillna('Missing'))  # Umwandlung und fehlende Werte ersetzen\n",
    "        label_encoders[col] = encoder  # Speichern des Encoders\n",
    "\n",
    "# 2. KNN-Imputation auf den numerischen Werten anwenden\n",
    "imputer = KNNImputer(n_neighbors=2)  # Anzahl der Nachbarn (k) einstellen\n",
    "df_imputed_categorical = pd.DataFrame(imputer.fit_transform(df_step2[categorical]), columns=categorical)\n",
    "\n",
    "# 3. Imputierte Werte zurück in kategorische Werte umwandeln\n",
    "for col, encoder in label_encoders.items():\n",
    "    df_imputed_categorical[col] = encoder.inverse_transform(df_imputed_categorical[col].round().astype(int))  # Rückumwandlung\n",
    "\n",
    "# Ausgabe des imputierten DataFrames\n",
    "df_imputed_categorical\n",
    "\n",
    "df_imputed_categorical.index = df_step2.index # adapt index of new dataframe to index of original dataframe\n",
    "df_step2[categorical] = df_imputed_categorical # replace numeric columns in original dataframe with imputed values\n",
    "\n",
    "print(df_step2.isnull().sum()) # check if all missing values in numeric columns were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step2, '2 Missing Value Handling')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step3 = df_step0.copy()\n",
    "\n",
    "#  Detection of outliers with IQR-method\n",
    "def detect_outliers_iqr(df):\n",
    "    df_outliers = df.copy()\n",
    "    for col in df.select_dtypes(include=np.number):  # Nur numerische Spalten\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_outliers[col] = df[col].apply(lambda x: np.nan if x < lower_bound or x > upper_bound else x)\n",
    "    return df_outliers\n",
    "\n",
    "\n",
    "#  Replacement of outliers mit NaN\n",
    "data_numeric_no_outliers = detect_outliers_iqr(df_step3[numeric])\n",
    "\n",
    "data_numeric_no_outliers.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Iterative Imputation for numeric columns\n",
    "imputer = IterativeImputer(max_iter=30, tol=1e-2, random_state=42, initial_strategy='median') #initial all missing values were replaced by median\n",
    "data_numeric_imputed = pd.DataFrame(imputer.fit_transform(data_numeric_no_outliers)) # creating a new dataframe with imputed values\n",
    "\n",
    "data_numeric_imputed.index = df_step3.index # adapt index of new dataframe to index of original dataframe\n",
    "#df_imputed_numeric = df_step3.copy() # create a copy of the original dataframe\n",
    "df_step3[numeric] = data_numeric_imputed # replace numeric columns in original dataframe with imputed values\n",
    "\n",
    "print(df_step3[numeric].isnull().sum()) # check if all missing values in numeric columns were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step3, '3 Outliers')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Transformation Normalverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step4 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the distribution of the numerical data with histograms\n",
    "%matplotlib inline\n",
    "hist = df_step4[numeric].hist(bins=30,figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_ohne_target = numeric.copy()\n",
    "numeric_ohne_target.remove('user_of_latest_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Hauptzweck dieser Transformation ist:\n",
    "Umwandlung nicht-normalverteilter Daten in normalverteilte Daten\n",
    "Reduzierung des Einflusses von Ausreißern\n",
    "Verbesserung der Performance von Machine Learning Modellen, die von normalverteilten Daten profitieren\n",
    "Standardisierung der Datenverteilung über alle numerischen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "#Erstellt einen Transformer, der die Daten in eine Normalverteilung überführt\n",
    "qt = QuantileTransformer(n_quantiles=25, output_distribution='normal', random_state=0)\n",
    "\n",
    "#Lernt die Transformation aus den Daten und wendet sie direkt an\n",
    "#Transformiert alle numerischen Spalten außer 'user_of_latest_model'\n",
    "#Das Ergebnis ist eine NumPy-Array mit den transformierten Werten\n",
    "trans_x = qt.fit_transform(df_step4[numeric_ohne_target])  \n",
    "\n",
    "#Wandelt die transformierte Array wieder in einen DataFrame um\n",
    "#Ersetzt die ursprünglichen Werte im DataFrame mit den transformierten Werten\n",
    "#Behält die Index-Struktur bei\n",
    "df_step4[numeric_ohne_target] = pd.DataFrame(trans_x, columns=numeric_ohne_target, index=df_step4.index)\n",
    "\n",
    "# Plot histograms for each numerical column to visualize the distribution\n",
    "df_step4[numeric].hist(bins=20, figsize=(10, 10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step4, '4 Transformation Normalverteilung')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Power Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step5 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize PowerTransformer (Yeo-Johnson is default, Box-Cox requires only positive data)\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)  \n",
    "\n",
    "# Apply the transformation to all numerical columns\n",
    "trans_x = pt.fit_transform(df_step5[numeric_ohne_target])  \n",
    "\n",
    "# Convert the transformed array back to a DataFrame and replace original numerical columns\n",
    "df_step5[numeric_ohne_target] = pd.DataFrame(trans_x, columns=numeric_ohne_target, index=df_step5.index)\n",
    "\n",
    "# Plot histograms for each numerical column to visualize the distribution\n",
    "df_step5[numeric].hist(bins=20, figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step5, '5 Power Transformation')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Min-Max-Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step6 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialisiere den MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Transformiere die numerischen Spalten (ohne das Label)\n",
    "scaled_features = scaler.fit_transform(df_step6[numeric_ohne_target])\n",
    "\n",
    "# Konvertiere die transformierten Daten zurück in einen DataFrame mit den ursprünglichen Spaltennamen\n",
    "df_step6[numeric_ohne_target] = pd.DataFrame(scaled_features, \n",
    "                                            columns=numeric_ohne_target, \n",
    "                                            index=df_step6.index)\n",
    "\n",
    "# Visualisiere die Verteilung der skalierten Daten\n",
    "df_step6[numeric].hist(bins=20, figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "# Optional: Überprüfung der Skalierung\n",
    "print(\"\\nMin-Max Werte nach der Skalierung:\")\n",
    "for column in numeric_ohne_target:\n",
    "    print(f\"{column}:\")\n",
    "    print(f\"Min: {df_step6[column].min():.2f}\")\n",
    "    print(f\"Max: {df_step6[column].max():.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step6, '6 MIN-MAX-Scaler')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 Standard-Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step7 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler transformiert die Daten so, dass:\n",
    "Mittelwert = 0\n",
    "Standardabweichung = 1\n",
    "Die Werte sind nicht auf einen bestimmten Bereich beschränkt\n",
    "Die Transformation erfolgt nach der Formel: z = (x - μ) / σ\n",
    "\n",
    "Erstellt eine Kopie von df_step4\n",
    "Führt die Standardisierung der numerischen Features durch\n",
    "Behält die Zielvariable unverändert\n",
    "Visualisiert die Verteilungen\n",
    "Überprüft die erfolgreiche Standardisierung durch Ausgabe von Mittelwert und Standardabweichung\n",
    "\n",
    "Die standardisierten Werte sollten nun:\n",
    "\n",
    "Einen Mittelwert nahe 0 haben\n",
    "Eine Standardabweichung nahe 1 haben\n",
    "Die ursprüngliche Verteilungsform beibehalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere den StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transformiere die numerischen Spalten (ohne das Label)\n",
    "scaled_features = scaler.fit_transform(df_step7[numeric_ohne_target])\n",
    "\n",
    "# Konvertiere die transformierten Daten zurück in einen DataFrame\n",
    "df_step7[numeric_ohne_target] = pd.DataFrame(scaled_features, \n",
    "                                            columns=numeric_ohne_target, \n",
    "                                            index=df_step7.index)\n",
    "\n",
    "# Visualisiere die Verteilung\n",
    "plt.figure(figsize=(10, 10))\n",
    "df_step7[numeric].hist(bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Überprüfe die Standardisierung\n",
    "print(\"\\nÜberprüfung der Standardisierung (Mean ≈ 0, Std ≈ 1):\")\n",
    "for column in numeric_ohne_target:\n",
    "    print(f\"\\n{column}:\")\n",
    "    print(f\"Mittelwert: {df_step7[column].mean():.4f}\")\n",
    "    print(f\"Standardabweichung: {df_step7[column].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step7, '7 Standard-Scaler')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8 Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step8 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Ordinal Encoding werden kategorische Variablen in numerische Werte umgewandelt, wobei jeder Kategorie eine eindeutige Zahl zugeordnet wird. Dies ist besonders sinnvoll, wenn die Kategorien eine natürliche Ordnung oder Rangfolge haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle kategorischen Spalten im DataFrame anzeigen\n",
    "categorical_columns = df_step8.select_dtypes(include=['object']).columns\n",
    "print(\"Kategorische Spalten im DataFrame:\")\n",
    "print(categorical_columns.tolist())\n",
    "\n",
    "# Für jede kategorische Spalte die unique Werte anzeigen\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nUnique Werte in {col}:\")\n",
    "    print(df_step8[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# ✅ Spalten mit natürlicher Ordnung für Ordinal Encoding\n",
    "ordinal_columns = ['subscription_type', 'goal_of_training', 'preferred_training_daytime']\n",
    "\n",
    "# Mapping für subscription_type (manuelles Ordinal Encoding)\n",
    "subscription_mapping = {'Free': 0, 'Basic': 1, 'Premium': 2}\n",
    "df_step8['subscription_type'] = df_step8['subscription_type'].map(subscription_mapping)\n",
    "\n",
    "# ✅ Manuelle Reihenfolge für OrdinalEncoder\n",
    "ordinal_mappings = [\n",
    "    ['Recreation', 'Fitness', 'Competition'],  # Reihenfolge für goal_of_training\n",
    "    ['Morning', 'Afternoon', 'Evening']  # Reihenfolge für preferred_training_daytime\n",
    "]\n",
    "\n",
    "# Ordinal Encoding anwenden\n",
    "ordinal_encoder = OrdinalEncoder(categories=ordinal_mappings)\n",
    "df_step8[['goal_of_training', 'preferred_training_daytime']] = ordinal_encoder.fit_transform(\n",
    "    df_step8[['goal_of_training', 'preferred_training_daytime']]\n",
    ")\n",
    "\n",
    "# 🔹 Ausgabe der Zuordnungen\n",
    "print(\"\\nZuordnungen für goal_of_training & preferred_training_daytime:\")\n",
    "for i, column in enumerate(['goal_of_training', 'preferred_training_daytime']):\n",
    "    print(f\"\\n{column}:\")\n",
    "    for j, category in enumerate(ordinal_encoder.categories_[i]):\n",
    "        print(f\"{category} -> {j}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step8, '8 Ordinal Encoding')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step9 Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step9 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führt ANOVA F-Test durch, um die statistische Signifikanz der Features zu bewerten. \n",
    "Nutzt Random Forest Feature Importance für eine modellbasierte Bewertung.\n",
    "Erstellt eine Korrelationsmatrix zur Identifizierung redundanter Features.\n",
    "Kombiniert die Ergebnisse zu einer Empfehlung.\n",
    "Behält nur die wichtigsten Features im finalen DataFrame.\n",
    "Die Entscheidung, welche Features behalten werden sollen, basiert auf:\n",
    "* Hoher Feature Importance\n",
    "* Statistischer Signifikanz (niedriger p-Wert im ANOVA Test)\n",
    "* Geringer Korrelation mit anderen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Features und Target definieren\n",
    "X = df_step9[numeric_ohne_target]  # Numerische Features ohne Label\n",
    "y = df_step9['user_of_latest_model']  # Zielvariable\n",
    "\n",
    "# 1. ANOVA F-Test für numerische Features\n",
    "f_selector = SelectKBest(f_classif, k='all')\n",
    "f_selector.fit(X, y)\n",
    "\n",
    "# Feature Scores aus ANOVA F-Test\n",
    "anova_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F_Score': f_selector.scores_,\n",
    "    'P_value': f_selector.pvalues_\n",
    "})\n",
    "anova_scores = anova_scores.sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(\"ANOVA F-Test Scores:\")\n",
    "print(anova_scores)\n",
    "\n",
    "# 2. Random Forest Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature Importance aus Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "})\n",
    "rf_importance = rf_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(rf_importance)\n",
    "\n",
    "# 3. Korrelationsmatrix für numerische Features\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Identifiziere stark korrelierte Features (z.B. > 0.8)\n",
    "high_correlation = np.where(np.abs(correlation_matrix) > 0.8)\n",
    "high_correlation = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y]) \n",
    "                   for x, y in zip(*high_correlation) if x != y and x < y]\n",
    "\n",
    "print(\"\\nStark korrelierte Features (>0.8):\")\n",
    "for feat1, feat2, corr in high_correlation:\n",
    "    print(f\"{feat1} - {feat2}: {corr:.2f}\")\n",
    "\n",
    "# Kombiniere die Ergebnisse für eine Empfehlung\n",
    "# Wähle Features basierend auf Importance und geringer Korrelation\n",
    "important_features = rf_importance[rf_importance['Importance'] > rf_importance['Importance'].mean()]['Feature'].tolist()\n",
    "\n",
    "print(\"\\nEmpfohlene Features basierend auf Importance und geringer Korrelation:\")\n",
    "print(important_features)\n",
    "\n",
    "# Optional: Erstelle einen neuen DataFrame nur mit den wichtigsten Features\n",
    "selected_features = important_features + ['user_of_latest_model']  # Füge Label hinzu\n",
    "df_step9 = df_step9[selected_features]\n",
    "\n",
    "# Visualisiere die Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(rf_importance['Feature'], rf_importance['Importance'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Feature Importance aus Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nun die Feature Selection der kategorischen Variablen:\n",
    "Nutzt die bereits vorhandenen kategorischen Spalten (categorical_columns)\n",
    "Führt das Ordinal Encoding durch\n",
    "Berechnet die Feature Importance für kategorische Features\n",
    "Visualisiert die Ergebnisse\n",
    "Wählt wichtige kategorische Features aus\n",
    "Kombiniert sie mit den bereits ausgewählten numerischen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zurück zum ursprünglichen DataFrame für die kategorische Feature Selection, da im vorherigen Schritt die numerischen ausgesondert wurden\n",
    "df_temp = df_step0.copy()\n",
    "\n",
    "# Encoding für kategorische Features\n",
    "encoder = OrdinalEncoder()\n",
    "X_cat = pd.DataFrame(encoder.fit_transform(df_temp[categorical_columns]), \n",
    "                    columns=categorical_columns)\n",
    "y_cat = df_temp['user_of_latest_model']\n",
    "\n",
    "# Random Forest für kategorische Features\n",
    "rf_cat = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_cat.fit(X_cat, y_cat)\n",
    "\n",
    "# Feature Importance für kategorische Features\n",
    "rf_importance_cat = pd.DataFrame({\n",
    "    'Feature': X_cat.columns,\n",
    "    'Importance': rf_cat.feature_importances_\n",
    "})\n",
    "rf_importance_cat = rf_importance_cat.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance (kategorische Features):\")\n",
    "print(rf_importance_cat)\n",
    "\n",
    "# Wichtige kategorische Features auswählen\n",
    "important_cat_features = rf_importance_cat[rf_importance_cat['Importance'] > \n",
    "                                         rf_importance_cat['Importance'].mean()]['Feature'].tolist()\n",
    "\n",
    "print(\"\\nWichtige kategorische Features:\")\n",
    "print(important_cat_features)\n",
    "\n",
    "# Visualisierung der kategorischen Feature Importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(rf_importance_cat['Feature'], rf_importance_cat['Importance'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Feature Importance - Kategorische Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Finalen DataFrame aktualisieren mit numerischen und kategorischen Features\n",
    "all_selected_features = selected_features + important_cat_features\n",
    "df_step9 = df_temp[all_selected_features]\n",
    "\n",
    "print(\"\\nFinale ausgewählte Features (numerisch + kategorisch):\")\n",
    "print(df_step9.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_step9, '9 Feature Selection')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardisiert die numerischen Daten\n",
    "Führt PCA durch\n",
    "Analysiert die erklärte Varianz\n",
    "Visualisiert die kumulierte erklärte Varianz\n",
    "Wählt die optimale Anzahl von Komponenten\n",
    "Erstellt einen neuen DataFrame mit:\n",
    "\n",
    "PCA-Komponenten für numerische Features\n",
    "Originalen kategorischen Features\n",
    "Target-Variable\n",
    "\n",
    "Schwellenwert für die erklärte Varianz (hier 90%) nach Bedarf anpassen.\n",
    "Die Feature Loadings am Ende zeigen, welche originalen Features am wichtigsten für jede Hauptkomponente sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_step10 = df_step0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1️⃣ Numerische Werte aus df_step10 extrahieren\n",
    "X = df_step10.select_dtypes(include=['int64', 'float64']).drop(columns=['user_of_latest_model'], errors='ignore')\n",
    "y = df_step10['user_of_latest_model']\n",
    "\n",
    "# 2️⃣ Standardisierung der numerischen Daten (wichtig für PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3️⃣ PCA anwenden (hier mit 2 Hauptkomponenten, kann angepasst werden)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 4️⃣ PCA-Ergebnis in einen neuen DataFrame umwandeln\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'], index=df_step10.index)\n",
    "\n",
    "# 5️⃣ Zielvariable wieder hinzufügen\n",
    "df_pca['user_of_latest_model'] = y.values  \n",
    "\n",
    "# 6️⃣ Ergebnis anzeigen\n",
    "print(\"Erklärte Varianz:\", pca.explained_variance_ratio_)\n",
    "df_pca\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logistic_regression(df_pca, '10 Feature Extraction')\n",
    "df_score_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung der Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib für bessere Lesbarkeit konfigurieren\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Zwei Linien plotten\n",
    "plt.plot(df_score_test.index, df_score_test['Accuracy'], marker='o', label='Accuracy', color='blue')\n",
    "plt.plot(df_score_test.index, df_score_test['F1-Score'], marker='o', label='F1-Score', color='red')\n",
    "\n",
    "# Beschriftungen und Titel\n",
    "plt.xlabel('Preprocessing Schritte')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Entwicklung von Accuracy und F1-Score über die Preprocessing Schritte')\n",
    "\n",
    "# Legende hinzufügen\n",
    "plt.legend()\n",
    "\n",
    "# Grid für bessere Lesbarkeit\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# X-Achsen-Labels rotieren für bessere Lesbarkeit\n",
    "plt.xticks(df_score_test.index, ['Step '+str(i) for i in df_score_test.index], rotation=45)\n",
    "\n",
    "# Layout optimieren\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot anzeigen\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
